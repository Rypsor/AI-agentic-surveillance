

import streamlit as st
import os
import re
import cv2
import json
import datetime
import tempfile
from PIL import Image
from ultralytics import YOLO
import numpy as np
from collections import deque

from dotenv import load_dotenv
import google.generativeai as genai



# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="Sistema de Vigilancia IA", layout="wide")

# --- 1. CONFIGURACI√ìN INICIAL Y CARGA DE MODELOS ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    st.error("No se encontr√≥ la clave GEMINI_API_KEY. Aseg√∫rate de crear un archivo .env.")
    st.stop()
genai.configure(api_key=GEMINI_API_KEY)



BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "output") # Renombrado para ser m√°s gen√©rico
CLIPS_DIR = os.path.join(OUTPUT_DIR, "clips") # Nueva carpeta para los videoclips
os.makedirs(CLIPS_DIR, exist_ok=True) # Crea ambas carpetas si no existen

# --- NUEVA FUNCI√ìN AUXILIAR PARA EXTRAER SEGMENTOS DE VIDEO ---
def extract_video_segment(input_path: str, output_path: str, start_frame: int, end_frame: int, fps: float):
    """
    Extrae un segmento de un video original (con todos sus fotogramas)
    y lo guarda en un nuevo archivo.
    """
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        print(f"Error: No se pudo abrir el video de entrada en {input_path}")
        return None

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_size = (width, height)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, frame_size)

    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    current_frame = start_frame
    while current_frame < end_frame:
        ret, frame = cap.read()
        if not ret: break
        writer.write(frame)
        current_frame += 1

    cap.release()
    writer.release()
    
    return output_path



# ==============================================================================
# ====== REEMPLAZA TU FUNCI√ìN load_models EXISTENTE CON ESTA ===================
# ==============================================================================

@st.cache_resource
def load_models():
    """
    Carga los modelos de IA una sola vez, leyendo los nombres de los modelos 
    de Gemini desde el archivo .env.
    """
    with st.spinner("Cargando modelos de IA (esto solo sucede la primera vez)..."):
        # Carga de modelos YOLO (sin cambios)
        intention_to_model_path = {
            "accidente": os.path.join(BASE_DIR, "weights", "best_accident.pt"),
            "fuego": os.path.join(BASE_DIR, "weights", "best_fire.pt"),
            "general": os.path.join(BASE_DIR, "weights", "best_general.pt")
        }
        models = {name: YOLO(path) for name, path in intention_to_model_path.items()}
        
        # --- CAMBIOS AQU√ç ---
        # 1. Leer los nombres de los modelos desde el archivo .env
        #    Se proporciona un valor por defecto ("gemini-1.5-flash") por si las variables no existen en .env
        vision_model_name = os.getenv("VISION_MODEL", "gemini-1.5-flash") # <<< NUEVO
        text_model_name = os.getenv("TEXT_MODEL", "gemini-1.5-flash")   # <<< NUEVO

        st.info(f"Cargando modelo de visi√≥n: `{vision_model_name}`") # Opcional: para depuraci√≥n
        st.info(f"Cargando modelo de texto: `{text_model_name}`")   # Opcional: para depuraci√≥n

        # 2. Inicializar los modelos de Gemini usando los nombres le√≠dos
        vision_model = genai.GenerativeModel(vision_model_name) # <<< MODIFICADO
        text_model = genai.GenerativeModel(text_model_name)     # <<< MODIFICADO
        
        return models, vision_model, text_model


detection_models, vision_model, text_model = load_models()


# --- 2. DEFINICI√ìN DE AGENTES DE IA ---

def interpret_and_dispatch(user_intention: str) -> dict:
    """Agente 1 Despachador: Decide qu√© pipeline usar y extrae los par√°metros."""
    
    # --- BASE DE CONOCIMIENTO AMPLIADA ---
    # Ahora incluye TODAS las clases de tus modelos y m√°s sin√≥nimos.
    knowledge_base = """
    ### Modelos y Clases (MAPA FUNDAMENTAL) ###
    - Modelo "accidente": Contiene las clases ['accident', 'moderate', 'severe']. Se especializa en detectar choques y su gravedad.
    - Modelo "fuego": Contiene las clases ['fire', 'smoke']. Se especializa en detectar incendios.
    - Modelo "general": Contiene todas las dem√°s clases: ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'train', 'truck', 'cat', 'dog'].

    ### Conceptos y Sin√≥nimos (TRADUCTOR) ###
    - "mascotas", "animales": ["dog", "cat"]
    - "veh√≠culos", "transporte": ["car", "bus", "truck", "train", "motorcycle", "bicycle"]
    - "personas", "gente", "intrusos": ["person"]
    - "incendio", "llama", "fuego": ["fire"]
    - "humo": ["smoke"]
    - "accidente", "choque", "colisi√≥n": ["accident", "severe"]
    - "carros", "coches", "autos": ["car"]
    - "camioneta", "cami√≥n": ["truck"]
    - "moto", "motocicleta": ["motorcycle"]
    - "bici", "bicicleta": ["bicycle"]
    - "transporte p√∫blico": ["bus", "train"]

    ### Reglas de Decisi√≥n Clave ###
    1.  **Prioridad de Modelos:** Si la solicitud menciona "fuego", "humo", "incendio" o "accidente", SIEMPRE debes usar los modelos especializados ("fuego" o "accidente") para esas clases.
    2.  **Default a 'search':** Si el usuario pide buscar algo con una propiedad visual (color, tama√±o, acci√≥n, estado como "estacionado", "roto"), el workflow es "search".
    3.  **Default a 'monitor':** Si el usuario usa palabras como "vigila", "alerta si", "av√≠same cuando", "monitorea", el workflow es "monitor".
    4. **Generalizaci√≥n:** nunca uses "general" para buscar "car" cuando de accidentes se trata, usa "accidente" directamente.
    """
    
    prompt = f"""
    Eres un despachador de IA experto que dirige las solicitudes a dos pipelines: "monitor" (vigilancia en tiempo real) y "search" (encontrar objetos espec√≠ficos).
    Tu √∫nica tarea es analizar la solicitud del usuario bas√°ndote en tu base de conocimiento y responder √öNICAMENTE con un objeto JSON v√°lido que contenga el pipeline y su configuraci√≥n. No a√±adas explicaciones ni texto adicional.

    ### Formato de Salida (Estricto) ###
    - Para "monitor": {{"workflow": "monitor", "config": {{"nombre_del_modelo": ["clase_1", "clase_2"]}}}}
    - Para "search": {{"workflow": "search", "config": {{"base_class": "nombre_de_la_clase", "specific_property": "descripci√≥n de la propiedad"}}}}

    ### Ejemplos Detallados ###
    - Solicitud: "vigila si hay mascotas o un accidente"
      JSON: {{"workflow": "monitor", "config": {{"general": ["dog", "cat"], "accidente": ["accident"]}}}}
    - Solicitud: "encuentra todos los camiones de color azul"
      JSON: {{"workflow": "search", "config": {{"base_class": "truck", "specific_property": "de color azul"}}}}
    - Solicitud: "quiero que busques cualquier indicio de fuego"
      JSON: {{"workflow": "search", "config": {{"base_class": "fire", "specific_property": "cualquier indicio"}}}}
    - Solicitud: "encuentra una bici que est√© estacionada"
      JSON: {{"workflow": "search", "config": {{"base_class": "bicycle", "specific_property": "que est√© estacionada"}}}}
    - Solicitud: "monitorea todos los veh√≠culos y si hay humo"
      JSON: {{"workflow": "monitor", "config": {{"general": ["car", "bus", "truck", "train", "motorcycle", "bicycle"], "fuego": ["smoke"]}}}}

    ### Base de Conocimiento ###
    {knowledge_base}
    ---
    Solicitud del Usuario: "{user_intention}"
    ---
    JSON de Respuesta:
    """
    try:
        # Usar un modelo de texto potente es clave aqu√≠
        # gemini-1.5-flash o superior es ideal
        response = text_model.generate_content(prompt)
        # Limpieza robusta de la respuesta para extraer solo el JSON
        cleaned_response = response.text.strip()
        match = re.search(r'\{.*\}', cleaned_response, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        else:
            st.error("El Agente Despachador no devolvi√≥ un JSON v√°lido.")
            st.text_area("Respuesta completa del modelo:", cleaned_response, height=150)
            return {}

    except Exception as e:
        st.error(f"Error cr√≠tico en el Agente Despachador: {e}")
        st.text_area("Respuesta del modelo que caus√≥ el error:", response.text if 'response' in locals() else "No se recibi√≥ respuesta.", height=150)
        return {}

def describe_scene_with_gemini(frames: list, user_intention: str, target_classes: list) -> dict:
    """Agente 2 (Guardia de Seguridad): Analiza una secuencia de fotogramas y genera un reporte JSON."""
    if not frames: 
        return {"error": "No se capturaron fotogramas para el an√°lisis."}
        
    pil_images = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in frames]

    # --- PROMPT MEJORADO CON ROL ESPEC√çFICO ---
    prompt = f"""
    Eres un guardia de seguridad experimentado y vigilante. Tu misi√≥n es analizar la siguiente secuencia de fotogramas de una c√°mara de seguridad para redactar un parte de situaci√≥n.

    **Contexto de la Alerta:**
    - **Intenci√≥n del Operador:** "{user_intention}"
    - **Detecci√≥n Autom√°tica Inicial:** Se han identificado los siguientes objetos de inter√©s: {target_classes}.

    **Tu Tarea:**
    Observa la secuencia de im√°genes y describe la escena de manera objetiva, clara y concisa. C√©ntrate en los hechos observables.

    **Formato de Salida Obligatorio:**
    Responde √öNICAMENTE con un objeto JSON v√°lido que siga esta estructura:
    {{
        "resumen_escena": "Una descripci√≥n breve y general de la situaci√≥n. Describe el entorno (ej. 'calle urbana de d√≠a', 'aparcamiento nocturno') y los actores principales.",
        "eventos_clave": [
            "Una lista de 2 a 4 eventos secuenciales que narran lo que ocurre. S√© espec√≠fico. Por ejemplo: 'Un coche rojo se aproxima a otro veh√≠culo detenido.', 'El coche rojo no frena e impacta por detr√°s.', 'Comienza a salir humo del cap√≥ de ambos veh√≠culos.'"
        ],
        "evaluacion_inicial": "Tu evaluaci√≥n profesional como guardia. Clasifica la situaci√≥n (ej. 'Falsa alarma', 'Incidente menor', 'Posible emergencia', 'Emergencia confirmada') y justifica brevemente por qu√©."
    }}
    """
    
    try:
        # El prompt va primero, seguido de la lista de im√°genes
        response = vision_model.generate_content([prompt] + pil_images)
        # Limpieza robusta del texto de respuesta para asegurar que sea un JSON v√°lido
        cleaned_response = response.text.strip()
        
        # Encuentra el inicio y el fin del JSON para evitar texto extra
        json_start = cleaned_response.find('{')
        json_end = cleaned_response.rfind('}') + 1
        if json_start != -1 and json_end != 0:
            json_str = cleaned_response[json_start:json_end]
            return json.loads(json_str)
        else:
            return {"error": "El modelo no devolvi√≥ un JSON v√°lido."}

    except Exception as e:
        # Devuelve el error espec√≠fico para facilitar la depuraci√≥n
        return {"error": f"Fallo en el reporte del Agente 2: {str(e)}"}

def verify_alarm_with_gemini(scene_report: dict, user_intention: str) -> dict:
    """
    Agente 3 (Jefe de Seguridad/Despachador): Analiza el reporte del guardia,
    decide si es una emergencia real, determina a qu√© autoridad notificar
    y redacta el mensaje de alerta.
    """
    if "error" in scene_report:
        # Si el reporte del Agente 2 ya tiene un error, lo propagamos.
        return {"veredicto": "ERROR_EN_REPORTE", "justificacion": scene_report["error"]}
        
    report_str = json.dumps(scene_report, indent=2, ensure_ascii=False)

    # --- PROMPT MEJORADO CON EL NUEVO ROL DE DESPACHADOR ---
    prompt = f"""
    Eres el Jefe de Seguridad de un centro de control. Has recibido el siguiente parte de situaci√≥n de uno de tus guardias.
    Tu misi√≥n es tomar una decisi√≥n final: determinar si la situaci√≥n requiere una intervenci√≥n externa, decidir qu√© autoridad contactar (si aplica) y redactar un mensaje de alerta claro y conciso.

    **Contexto de la Tarea:**
    - **Intenci√≥n Original del Operador:** "{user_intention}"
    - **Parte de Situaci√≥n del Guardia (Agente 2):**
    {report_str}

    **Tus Responsabilidades:**
    1.  **Analiza el Veredicto Inicial:** Eval√∫a la conclusi√≥n de tu guardia a la luz de la intenci√≥n original.
    2.  **Toma la Decisi√≥n Final:** Emite un veredicto. Los veredictos posibles son: 'ALARMA_CONFIRMADA', 'INCIDENTE_MENOR' o 'FALSA_ALARMA'.
    3.  **Determina la Autoridad Competente:** Si es 'ALARMA_CONFIRMADA', decide a qui√©n notificar. Las opciones son: 'Polic√≠a', 'Bomberos', 'Servicios M√©dicos', 'Control de Animales' o 'Ninguna'.
    4.  **Redacta el Mensaje de Alerta:** Si se notifica a una autoridad, escribe un mensaje breve y directo para ellos.

    **Formato de Salida Obligatorio:**
    Responde √öNICAMENTE con un objeto JSON v√°lido que siga esta estructura. No a√±adas texto fuera del JSON.

    {{
      "veredicto_final": "Tu decisi√≥n final ('ALARMA_CONFIRMADA', 'INCIDENTE_MENOR', 'FALSA_ALARMA').",
      "justificacion": "Una explicaci√≥n breve y profesional de por qu√© tomaste esa decisi√≥n, bas√°ndote en el reporte.",
      "autoridad_a_notificar": "La autoridad competente (por ejemplo 'Polic√≠a', o 'Bomberos', o 'Param√©dicos', etc.) o 'Ninguna' si no es una emergencia.",
      "mensaje_de_alerta": "El mensaje para la autoridad, describe la escena muy brevemente y recomendaciones de seguridad brevemente. Si la autoridad es 'Ninguna', este campo debe ser 'No se requiere acci√≥n.'."
    }}

    **Ejemplo para un incendio:**
    {{
      "veredicto_final": "ALARMA_CONFIRMADA",
      "justificacion": "El reporte confirma la presencia de humo y llamas, lo que constituye una emergencia de incendio.",
      "autoridad_a_notificar": "Bomberos",
      "mensaje_de_alerta": "Alerta de Incendio: Se detecta fuego activo. Se observa humo denso y llamas visibles. Se requiere intervenci√≥n inmediata."
    }}
    """
    
    try:
        response = text_model.generate_content(prompt)
        # Usamos la misma l√≥gica de limpieza robusta para el JSON
        cleaned_response = response.text.strip()
        json_start = cleaned_response.find('{')
        json_end = cleaned_response.rfind('}') + 1
        if json_start != -1 and json_end != 0:
            json_str = cleaned_response[json_start:json_end]
            return json.loads(json_str)
        else:
            return {"veredicto_final": "ERROR_DE_MODELO", "justificacion": "El modelo no devolvi√≥ un JSON v√°lido.", "autoridad_a_notificar": "Ninguna", "mensaje_de_alerta": ""}

    except Exception as e:
        return {"veredicto_final": "ERROR_DE_CODIGO", "justificacion": str(e), "autoridad_a_notificar": "Ninguna", "mensaje_de_alerta": ""}

def verify_property_with_gemini(image: Image, base_class: str, specific_property: str) -> bool:
    """Agente de B√∫squeda: Verifica si una imagen cumple una propiedad espec√≠fica."""
    
    # --- PROMPT MEJORADO ---
    # Este nuevo prompt es m√°s directo, le da un rol al modelo y le exige una respuesta √∫nica.
    prompt = f"""
    Tu √∫nica tarea es analizar la imagen y responder a una pregunta con una sola palabra.

    Pregunta: ¬øEl objeto en esta imagen es un/a '{base_class}' que es/tiene la caracter√≠stica '{specific_property}'?

    Instrucci√≥n estricta: Responde √öNICAMENTE con la palabra "S√ç" o la palabra "NO".
    No a√±adas ninguna explicaci√≥n, frase, o puntuaci√≥n. Tu respuesta debe ser una de esas dos palabras y nada m√°s.
    """
    
    try:
        response = vision_model.generate_content([prompt, image])
        
        # --- L√ìGICA DE COMPROBACI√ìN MEJORADA ---
        # Usamos .strip() para eliminar espacios en blanco y comparamos la respuesta exacta.
        # Esto es m√°s seguro que usar 'in' por si el modelo respondiera algo como "NO SE VE CLARO".
        return response.text.strip().upper() == "S√ç"
        
    except Exception as e:
        # Es buena pr√°ctica registrar el error si algo sale mal durante la llamada a la API
        st.warning(f"Error en la API de Gemini durante la verificaci√≥n: {e}")
        return False


# --- 3. DEFINICI√ìN DE PIPELINES ---

# No olvides importar 'deque' al principio de tu archivo app.py
from collections import deque

def run_monitor_pipeline(video_path: str, user_intention: str, surveillance_config: dict, confidence_threshold: float, frame_processing_rate: int, capture_duration: int, progress_bar, status_text):
    """
    Pipeline de monitoreo que contin√∫a el an√°lisis tras un incidente menor o falsa alarma,
    y se detiene solo ante una alarma confirmada o un error.
    """
    st.info(f"üß† **Pipeline de Monitoreo Activado:** Configuraci√≥n: `{json.dumps(surveillance_config)}`")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        st.error(f"Error al abrir el archivo de video en la ruta: {video_path}")
        return

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    frames_to_skip_for_analysis = max(1, int(fps * 0.5))
    num_frames_for_analysis = int(capture_duration * fps / frames_to_skip_for_analysis)
    num_before_for_analysis = num_frames_for_analysis // 2
    analysis_frame_buffer = deque(maxlen=num_before_for_analysis)

    frame_count = 0
    stop_processing = False # Controla la detenci√≥n del bucle principal
    events_found_count = 0

    while cap.isOpened() and not stop_processing:
        ret, frame = cap.read()
        if not ret: break

        # El buffer se llena continuamente con frames de alta calidad para el an√°lisis
        if frame_count % frames_to_skip_for_analysis == 0:
            analysis_frame_buffer.append(frame.copy())
        
        frame_count += 1
        
        # Actualizar la barra de progreso
        if frame_count % 10 == 0: 
            progress_bar.progress(frame_count / total_frames, text=f"Monitoreando... (Frame {frame_count}/{total_frames})")

        # Saltar frames para el procesamiento con YOLO
        if frame_count % frame_processing_rate != 0:
            continue
        
        for model_name, target_classes in surveillance_config.items():
            if model_name not in detection_models: continue
            
            model = detection_models[model_name]
            results = model(frame, conf=confidence_threshold, verbose=False)
            
            for box in results[0].boxes:
                class_id = int(box.cls[0])
                detected_class_name = model.names[class_id]

                if detected_class_name.lower() in [cls.lower() for cls in target_classes]:
                    events_found_count += 1
                    
                    with st.spinner(f"üö® ¬°Posible evento '{detected_class_name}'! Analizando con IA (Evento #{events_found_count})..."):
                        
                        # Recopilar fotogramas para el an√°lisis de IA (contexto antes y despu√©s)
                        collected_analysis_frames = list(analysis_frame_buffer)
                        num_after_to_collect = num_frames_for_analysis - len(collected_analysis_frames)
                        
                        temp_cap = cv2.VideoCapture(video_path)
                        temp_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)
                        for _ in range(num_after_to_collect):
                            for _ in range(frames_to_skip_for_analysis):
                                temp_cap.read()
                            ret_capture, next_frame = temp_cap.read()
                            if not ret_capture: break
                            collected_analysis_frames.append(next_frame.copy())
                        temp_cap.release()

                        # An√°lisis completo de los agentes
                        scene_report = describe_scene_with_gemini(collected_analysis_frames, user_intention, target_classes)
                        decision_result = verify_alarm_with_gemini(scene_report, user_intention)
                        veredicto = decision_result.get("veredicto_final", "ERROR_DESCONOCIDO")

                    # --- DECISI√ìN GRANULAR: CONTINUAR, REGISTRAR O DETENER ---
                    justificacion = decision_result.get("justificacion", "No se proporcion√≥ justificaci√≥n.")

                    if veredicto == "FALSA_ALARMA":
                        with st.container():
                            st.success(f"‚úîÔ∏è **Falsa Alarma Descartada** (en frame {frame_count}). Reanudando monitoreo.")
                            st.info(f"**Justificaci√≥n:** {justificacion}")
                            st.divider()
                        break 

                    elif veredicto == "INCIDENTE_MENOR":
                        with st.container():
                            st.warning(f"‚ö†Ô∏è **Incidente Menor Registrado** (en frame {frame_count}). El monitoreo contin√∫a.")
                            st.info(f"**Justificaci√≥n:** {justificacion}")
                            annotated_frame = results[0].plot()
                            st.image(annotated_frame, caption=f"Frame del incidente menor: '{detected_class_name}'", channels="BGR")
                            st.divider()
                        break 

                    else: # ALARMA_CONFIRMADA o ERROR
                        stop_processing = True # ¬°CLAVE! Esto detendr√° el bucle principal 'while'.
                        
                        st.error(f"üö® ¬°ALARMA CONFIRMADA! Evento: '{detected_class_name}' en frame {frame_count}. Deteniendo an√°lisis.")
                        annotated_frame = results[0].plot()
                        st.image(annotated_frame, caption=f"Fotograma clave de la alarma: '{detected_class_name}'", channels="BGR", use_column_width=True)

                        # Extraer el videoclip del evento real
                        clip_path = None
                        with st.spinner("üíæ Extrayendo videoclip del evento..."):
                            clip_total_frames = int(capture_duration * fps)
                            start_clip_frame = max(0, frame_count - (clip_total_frames // 2))
                            end_clip_frame = min(total_frames, frame_count + (clip_total_frames // 2))
                            
                            clip_filename = f"detection_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                            clip_path = os.path.join(CLIPS_DIR, clip_filename)
                            extract_video_segment(video_path, clip_path, start_clip_frame, end_clip_frame, fps)

                        # Mostrar resultados finales
                        st.subheader("üìù Reporte del Agente 2 (Guardia)")
                        if "error" not in scene_report: st.json(scene_report)
                        else: st.error(f"Fallo en el reporte del Agente 2: {scene_report['error']}")

                        st.subheader("‚öñÔ∏è Decisi√≥n Final del Jefe de Seguridad (Agente 3)")
                        autoridad = decision_result.get("autoridad_a_notificar", "Ninguna")
                        mensaje = decision_result.get("mensaje_de_alerta", "No hay mensaje.")

                        if "ERROR" in veredicto:
                            st.error(f"**Veredicto: {veredicto}**")
                        else: # ALARMA_CONFIRMADA
                            st.error(f"**Veredicto: {veredicto}**")
                            st.info(f"**Justificaci√≥n:** {justificacion}")
                            st.warning(f"**Acci√≥n Inmediata: Notificar a `{autoridad}`**")
                            st.code(mensaje, language="text")
                        
                        if clip_path and os.path.exists(clip_path):
                            st.divider()
                            with open(clip_path, "rb") as file:
                                st.download_button(
                                    label="üì• Descargar Videoclip de la Alarma (.mp4)",
                                    data=file,
                                    file_name=clip_filename,
                                    mime="video/mp4",
                                    use_container_width=True
                                )
                        
                        break # Rompemos el bucle de 'boxes'
            
            if stop_processing:
                break
    
    cap.release()
    if not stop_processing:
        st.success(f"‚úÖ Monitoreo completo del video. Se analizaron {events_found_count} eventos potenciales.")
    
    progress_bar.empty()
    status_text.text("An√°lisis finalizado.")


def run_search_pipeline(video_path: str, base_class: str, specific_property: str, confidence_threshold: float, frame_processing_rate: int, progress_bar, status_text):
    st.info(f" **Pipeline de B√∫squeda Activado:**.")
    st.info(f" **Configuraci√≥n:** Analizando 1 de cada **{frame_processing_rate}** fotogramas.") # Informar al usuario
    
    class_to_model_map = {
        "person": "general", "car": "general", "bus": "general", "truck": "general",
        "train": "general", "dog": "general", "cat": "general",
        "accident": "accidente", "fire": "fuego"
    }
    model_name = class_to_model_map.get(base_class)
    if not model_name:
        st.error(f"No hay modelo para '{base_class}'."); return
    
    model = detection_models[model_name]
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    detected_objects = []

    status_text.text(f"Fase 1/3: Detectando '{base_class}'...")
    class_index = list(model.names.values()).index(base_class)
    
    # Bucle principal para recorrer el video
    for frame_idx in range(total_frames):
        ret, frame = cap.read()
        if not ret: break
        
        progress_bar.progress((frame_idx + 1) / total_frames)

        # --- TODO ESTE BLOQUE AHORA EST√Å DENTRO DEL 'FOR' ---
        
        # Condici√≥n de salto
        if frame_idx % frame_processing_rate != 0:
            continue # Saltar al siguiente fotograma si no toca procesar este

        # El resto del c√≥digo solo se ejecuta si el frame no se salta
        results = model(frame, classes=[class_index], conf=confidence_threshold, verbose=False)
        
        for box in results[0].boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            # Se a√±ade el objeto recortado y el √≠ndice del frame
            detected_objects.append({"image": frame[y1:y2, x1:x2], "frame_idx": frame_idx})
    
    # --- FIN DEL BUCLE 'FOR' ---

    cap.release()
    if not detected_objects:
        st.success(f"No se detect√≥ ning√∫n '{base_class}' con la configuraci√≥n actual."); 
        progress_bar.empty(); status_text.text("An√°lisis finalizado."); 
        return

    # El resto de la funci√≥n (Fase 2 y 3) no necesita cambios
    status_text.text(f"Fase 2/3: Seleccionando muestras...")
    num_to_sample = min(len(detected_objects), 2)
    indices_to_sample = np.linspace(0, len(detected_objects) - 1, num_to_sample, dtype=int)
    sampled_objects = [detected_objects[i] for i in indices_to_sample]
    
    status_text.text(f"Fase 3/3: Verificando {len(sampled_objects)} muestras con Gemini...")
    final_matches = []
    cols = st.columns(5)
    col_idx = 0
    for i, item in enumerate(sampled_objects):
        progress_bar.progress((i + 1) / len(sampled_objects))
        pil_image = Image.fromarray(cv2.cvtColor(item["image"], cv2.COLOR_BGR2RGB))
        with st.spinner(f"Analizando muestra {i+1}..."):
            if verify_property_with_gemini(pil_image, base_class, specific_property):
                final_matches.append(item)
                with cols[col_idx % 5]:
                    st.image(item["image"], channels="BGR", caption=f"Frame {item['frame_idx']}")
                col_idx += 1

    progress_bar.empty(); status_text.text("An√°lisis finalizado.")
    if not final_matches: st.success("B√∫squeda completa. No se encontraron coincidencias.")
    else: st.success(f"B√∫squeda completa. Se encontraron {len(final_matches)} coincidencias.")





# --- 4. L√ìGICA PRINCIPAL DE LA APP ---
st.title("üëÅÔ∏è Sistema de Vigilancia con Agentes de IA")

# --- GENERACI√ìN DE LA DESCRIPCI√ìN EST√ÅTICA ---

# 1. Este diccionario contiene el texto exacto que quieres mostrar.
# Ser√° la √∫nica fuente para la lista de capacidades.
categories_to_display = {
    "Eventos Cr√≠ticos": ['accidentes automovil√≠sticos', 'choques', 'etc'],
    "Personas y Animales": ['personas', 'perros', 'gatos'],
    "Veh√≠culos": ['coches', 'autobuses', 'camiones', 'trenes', 'motos', 'bicicletas']
}

# 2. Construimos el texto del markdown directamente desde el diccionario.
# NO hay ninguna conexi√≥n con las clases reales de los modelos.
capabilities_markdown = ""
for category_name, keyword_list in categories_to_display.items():
    # Simplemente unimos la lista de palabras que definiste arriba.
    capabilities_markdown += f"- **{category_name}:** {', '.join(keyword_list)}.\n"

# 3. Mostrar el t√≠tulo y la descripci√≥n completa usando el texto que acabamos de generar.
# Este es el texto exacto que proporcionaste en tu mensaje anterior.
st.markdown(f"""
Este es un sistema avanzado que utiliza un modelo de visi√≥n artificial (YOLOv5m) con una cadena de tres agentes de Inteligencia Artificial para analizar videos. 
Puedes usarlo de dos formas principales:

- **Modo Monitoreo:** Vigila un video en tiempo real para detectar eventos espec√≠ficos (ej. informa de la presencia de choques vehiculares).
- **Modo B√∫squeda:** Busca objetos que cumplan con una caracter√≠stica particular. (ej. encuentra algun cami√≥n de color azul).

**Capacidades del Sistema:**
Nuestros modelos pueden detectar una amplia gama de elementos, incluyendo:
{capabilities_markdown}
**Para comenzar:** Carga un video, describe tu tarea y haz clic en "Procesar Video".
""")

st.divider()

# El resto del c√≥digo de la interfaz va aqu√≠...


# ...
col1, col2 = st.columns([2, 3])
# ...

col1, col2 = st.columns([2, 3])
with col1:
    st.header("1. Carga tu Video")
    uploaded_file = st.file_uploader("Selecciona un archivo", type=["mp4", "mov", "avi"])

with col2:
    st.header("2. Define la Tarea y Configuraci√≥n")
    user_intention = st.text_area(
        label="Describe la tarea:\nEj: 'vigila si hay un accidente' o 'revisa si hay algun camioneta roja'",
        value="c√°mara de seguridad, informa de la presencia de cualquier persona.", # Un valor por defecto m√°s √∫til
        height=100
    )
    sub_col1, sub_col2, sub_col3 = st.columns(3)
    with sub_col1:
        confidence = st.slider("Confianza Detecci√≥n", 0.1, 1.0, 0.4, 0.05)
    with sub_col2:
        # Modifiqu√© el help text para que sea m√°s claro
        frame_skip = st.slider("Analizar 1/N frames", 15, 300, 5, help="Afecta a ambos modos: Monitoreo y B√∫squeda")
    with sub_col3:
        capture_duration_option = st.selectbox("Duraci√≥n Captura", ["5 segundos", "10 segundos"], help="Solo para modo Monitoreo")
        capture_duration = int(capture_duration_option.split(" ")[0])

if st.button(" Procesar Video", use_container_width=True):
    # --- PASO 1: Validaci√≥n de entradas del usuario ---
    if not uploaded_file:
        st.warning("Por favor, sube un archivo de video.")
    elif not user_intention or not user_intention.strip():
        st.warning("Por favor, describe una intenci√≥n de vigilancia.")
    else:
        # Si las validaciones son correctas, se ejecuta el resto.
        
        # --- PASO 2: Guardar el video y preparar la UI ---
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tfile:
            tfile.write(uploaded_file.read())
            video_path = tfile.name
        
        st.header(" Resultados del An√°lisis")
        progress_bar = st.progress(0, text="Iniciando...")
        status_text = st.empty()
        
        # --- PASO 3: Llamar al Agente 1 (Despachador) ---
        # Esta llamada se hace UNA SOLA VEZ, al principio.
        dispatch_result = interpret_and_dispatch(user_intention)

        # --- PASO 4: Ejecutar el pipeline correspondiente ---
        if not dispatch_result or "workflow" not in dispatch_result:
            st.error("El Agente Despachador no pudo determinar un flujo de trabajo. Intenta reformular tu solicitud.")
            status_text.text("Error.")
            progress_bar.empty()
        
        elif dispatch_result["workflow"] == "monitor":
            run_monitor_pipeline(
                video_path=video_path,
                user_intention=user_intention,
                surveillance_config=dispatch_result["config"],
                confidence_threshold=confidence,
                frame_processing_rate=frame_skip,
                capture_duration=capture_duration,
                progress_bar=progress_bar,
                status_text=status_text
            )
        
        elif dispatch_result["workflow"] == "search":
            config = dispatch_result["config"]
            run_search_pipeline(
                video_path=video_path,
                base_class=config.get("base_class", ""),
                specific_property=config.get("specific_property", ""),
                confidence_threshold=confidence,
                frame_processing_rate=frame_skip, # Pasamos el par√°metro
                progress_bar=progress_bar,
                status_text=status_text
            )
        
        # --- PASO 5: Limpiar el archivo de video temporal ---
        os.remove(video_path)